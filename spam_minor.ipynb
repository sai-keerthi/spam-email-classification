{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YGy9RzTJzahy",
    "outputId": "d1ef6f1e-c8db-4996-f18c-57b1674ad936"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\perik\\anaconda3\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\perik\\\\AppData\\\\Local\\\\Temp\\\\pip-install-xjyl82cd\\\\wordcloud_48bff27a24924808b885ddfae44f085a\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\perik\\\\AppData\\\\Local\\\\Temp\\\\pip-install-xjyl82cd\\\\wordcloud_48bff27a24924808b885ddfae44f085a\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\perik\\AppData\\Local\\Temp\\pip-wheel-abotnfag'\n",
      "       cwd: C:\\Users\\perik\\AppData\\Local\\Temp\\pip-install-xjyl82cd\\wordcloud_48bff27a24924808b885ddfae44f085a\\\n",
      "  Complete output (20 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.9\n",
      "  creating build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\color_from_image.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\tokenization.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\wordcloud.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\wordcloud_cli.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\_version.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\__init__.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\__main__.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\stopwords -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\DroidSansMono.ttf -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  UPDATING build\\lib.win-amd64-3.9\\wordcloud/_version.py\n",
      "  set build\\lib.win-amd64-3.9\\wordcloud/_version.py to '1.8.1'\n",
      "  running build_ext\n",
      "  building 'wordcloud.query_integral_image' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for wordcloud\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\perik\\anaconda3\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\perik\\\\AppData\\\\Local\\\\Temp\\\\pip-install-xjyl82cd\\\\wordcloud_48bff27a24924808b885ddfae44f085a\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\perik\\\\AppData\\\\Local\\\\Temp\\\\pip-install-xjyl82cd\\\\wordcloud_48bff27a24924808b885ddfae44f085a\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\perik\\AppData\\Local\\Temp\\pip-record-xijic8z_\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\perik\\anaconda3\\Include\\wordcloud'\n",
      "         cwd: C:\\Users\\perik\\AppData\\Local\\Temp\\pip-install-xjyl82cd\\wordcloud_48bff27a24924808b885ddfae44f085a\\\n",
      "    Complete output (20 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib.win-amd64-3.9\n",
      "    creating build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\color_from_image.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\tokenization.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\wordcloud.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\wordcloud_cli.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\_version.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\__init__.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\__main__.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\stopwords -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\DroidSansMono.ttf -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    UPDATING build\\lib.win-amd64-3.9\\wordcloud/_version.py\n",
      "    set build\\lib.win-amd64-3.9\\wordcloud/_version.py to '1.8.1'\n",
      "    running build_ext\n",
      "    building 'wordcloud.query_integral_image' extension\n",
      "    error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'C:\\Users\\perik\\anaconda3\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\perik\\\\AppData\\\\Local\\\\Temp\\\\pip-install-xjyl82cd\\\\wordcloud_48bff27a24924808b885ddfae44f085a\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\perik\\\\AppData\\\\Local\\\\Temp\\\\pip-install-xjyl82cd\\\\wordcloud_48bff27a24924808b885ddfae44f085a\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\perik\\AppData\\Local\\Temp\\pip-record-xijic8z_\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\perik\\anaconda3\\Include\\wordcloud' Check the logs for full command output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Using cached wordcloud-1.8.1.tar.gz (220 kB)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\perik\\anaconda3\\lib\\site-packages (from wordcloud) (1.20.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\perik\\anaconda3\\lib\\site-packages (from wordcloud) (8.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\perik\\anaconda3\\lib\\site-packages (from wordcloud) (3.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\perik\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\perik\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\perik\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\perik\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six in c:\\users\\perik\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.16.0)\n",
      "Building wheels for collected packages: wordcloud\n",
      "  Building wheel for wordcloud (setup.py): started\n",
      "  Building wheel for wordcloud (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for wordcloud\n",
      "Failed to build wordcloud\n",
      "Installing collected packages: wordcloud\n",
      "    Running setup.py install for wordcloud: started\n",
      "    Running setup.py install for wordcloud: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\perik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\perik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from nltk.tokenize import word_tokenize\n",
    "!pip install wordcloud\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop=stopwords.words(\"english\")\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gHbLoQI9zpC6",
    "outputId": "49c9cca1-85f8-4b50-c453-89e66648474d"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/spam_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15948/2582909694.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"/content/spam_data.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/spam_data.csv'"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv(r\"/content/spam_data.csv\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "8egsgcsbahz8",
    "outputId": "3ae56670-fdc6-4c33-9dbb-8c72478098dc"
   },
   "outputs": [],
   "source": [
    "data.groupby('Category').describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "peQja7LVakuW",
    "outputId": "773d6960-2bbf-45f4-e84f-4a8a19972461"
   },
   "outputs": [],
   "source": [
    "data.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "eBShbaGeaoG4",
    "outputId": "9b4e6d44-1cbf-4a2f-8c51-10b08d9ddd05"
   },
   "outputs": [],
   "source": [
    "data=data.dropna()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "0paCvci3arUZ",
    "outputId": "40c2dde5-5bc0-4244-afde-be2832565ff6"
   },
   "outputs": [],
   "source": [
    "data[\"Category\"].value_counts().plot(kind = 'pie', explode = [0, 0.1], figsize = (6, 6))\n",
    "plt.ylabel(\"Spam vs Ham\")\n",
    "plt.legend([\"Ham\", \"Spam\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "id": "YsoVKTkbaumv",
    "outputId": "6e21452a-3124-4db0-f69b-a4f744843377"
   },
   "outputs": [],
   "source": [
    "print(data.dtypes)\n",
    "data['length']=data.Message.apply(len)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "id": "SqWq939Oax1W",
    "outputId": "08304fd3-c748-4de3-ab14-7b60648fffa6"
   },
   "outputs": [],
   "source": [
    "data.hist(column='length', by='Category', bins=50,figsize=(11,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDBX9S_CbBhI"
   },
   "outputs": [],
   "source": [
    "spam_msgs=data[data['Category']=='spam'].Message\n",
    "ham_msgs=data[data['Category']=='ham'].Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aVO6r-ncbd1L",
    "outputId": "aab737e2-2edc-4b59-bf00-ef17cdf10310"
   },
   "outputs": [],
   "source": [
    "spam_words = []\n",
    "ham_words = []\n",
    "\n",
    "def extractSpamWords(spamMessages):\n",
    "    global spam_words\n",
    "    words = [word for word in word_tokenize(spamMessages)]\n",
    "    spam_words = spam_words + words\n",
    "    \n",
    "def extractHamWords(hamMessages):\n",
    "    global ham_words\n",
    "    words = [word for word in word_tokenize(hamMessages) ]\n",
    "    ham_words = ham_words + words\n",
    "\n",
    "spam_msgs.apply(extractSpamWords)\n",
    "ham_msgs.apply(extractHamWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cPkQu7vTbiyi",
    "outputId": "0b8e4af2-5ff6-4f20-ce41-9495a008f893"
   },
   "outputs": [],
   "source": [
    "ham_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qPgasQy3bnqi",
    "outputId": "25399a65-8465-4edf-9925-3e7b2c61e300"
   },
   "outputs": [],
   "source": [
    "!pip install wordcloud\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "ujr6AfcsbrTk",
    "outputId": "2a8eccc0-267d-4b53-c067-2752d60a4a0e"
   },
   "outputs": [],
   "source": [
    "spam_wordcloud = WordCloud(width=600, height=400).generate(\" \".join(spam_words))\n",
    "plt.figure( figsize=(10,8), facecolor='k')\n",
    "plt.imshow(spam_wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "ZnilOe_1bsfE",
    "outputId": "e178cbf2-6adb-4d65-f3d1-06eff1bd9271"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "ham_wordcloud = WordCloud(width=600, height=400).generate(\" \".join(ham_words))\n",
    "plt.figure( figsize=(10,8))\n",
    "plt.imshow(ham_wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UWy8cmgKzu-M"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "def text_preprocessing(text):\n",
    "    text=text.translate(str.maketrans('','',string.punctuation))\n",
    "    text=[i.lower() for i in text.split() if i.lower() not in stop]\n",
    "    return \" \".join(text)\n",
    "data['Message']=data['Message'].apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3W376QMz4H0"
   },
   "outputs": [],
   "source": [
    "def encodeCategory(cat):\n",
    "    if cat == \"spam\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "data[\"Category\"] = data[\"Category\"].apply(encodeCategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2tRCZ-xzz91M",
    "outputId": "4d51b4b3-a90b-4deb-9677-2a6cd196f1e8"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X=cv.fit_transform(data[\"Message\"])\n",
    "print (X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5SXjObBw1P-E"
   },
   "outputs": [],
   "source": [
    "X = cv.fit_transform(data[\"Message\"]).toarray()\n",
    "Y=data['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BEotqSWP1b3j"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_reg=LogisticRegression(solver='liblinear',penalty='l1')\n",
    "logistic_reg.fit(X_train,y_train)\n",
    "lrprediction=logistic_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "# performance of logisctic regression\n",
    "print(\"Performance of logistic regression:\")\n",
    "print()\n",
    "print('classification report:\\n',classification_report(y_test,lrprediction))\n",
    "print()\n",
    "print(\"confuion matrix:\\n\",confusion_matrix(y_test,lrprediction))\n",
    "print()\n",
    "print(\"Accuracy score:\",accuracy_score(y_test,lrprediction))\n",
    "print(\"\\n\\nHeatmap on prediction:\\n\")\n",
    "sns.heatmap(confusion_matrix(y_test,lrprediction),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nbclassifier=MultinomialNB()\n",
    "nbclassifier.fit(X_train,y_train)\n",
    "nbprediction=nbclassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance of Naive bayes \n",
    "print(\"Performance of Naive bayes:\")\n",
    "print()\n",
    "print('classification report:\\n',classification_report(y_test,nbprediction))\n",
    "print()\n",
    "print(\"confuion matrix:\\n\",confusion_matrix(y_test,nbprediction))\n",
    "print()\n",
    "print(\"Accuracy score:\",accuracy_score(y_test,nbprediction))\n",
    "print(\"\\n\\nHeatmap on prediction:\\n\")\n",
    "sns.heatmap(confusion_matrix(y_test,nbprediction),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ekwBzlW1gz9"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "rf_model = rf.fit(X_train,y_train)\n",
    "y_pred_rf=rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 707
    },
    "id": "mEf-S6Bb12ze",
    "outputId": "41dc814a-4220-466b-aa75-720bfd86ebd2"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score,precision_score\n",
    "import seaborn as sns\n",
    "\n",
    "# performance of random forest\n",
    "print(\"Performance of random forest:\")\n",
    "print()\n",
    "print('classification report:\\n',classification_report(y_test,y_pred_rf))\n",
    "print()\n",
    "print(\"confuion matrix:\\n\",confusion_matrix(y_test,y_pred_rf))\n",
    "print()\n",
    "print(\"Accuracy score:\",accuracy_score(y_test,y_pred_rf))\n",
    "print('precision score:',precision_score(y_test,y_pred_rf))\n",
    "print(\"\\n\\nHeatmap on prediction:\\n\")\n",
    "sns.heatmap(confusion_matrix(y_test,y_pred_rf),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_rGEfil5TQ8z"
   },
   "outputs": [],
   "source": [
    "final_estimator=RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MpwEWZQmTQ81"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOvhCfl5TQ81"
   },
   "outputs": [],
   "source": [
    "abc = AdaBoostClassifier(n_estimators=50, random_state=2)\n",
    "etc = ExtraTreesClassifier(n_estimators=50, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 707
    },
    "id": "xrdX9j5hTQ82",
    "outputId": "21ac7ced-d48c-4484-d350-be1856231752"
   },
   "outputs": [],
   "source": [
    "estimators=[('abc',abc ), ('etc', etc)]\n",
    "clf = StackingClassifier(estimators=estimators, final_estimator=final_estimator)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "# performance of stacked classifier\n",
    "print(\"Performance of stacked classifier:\")\n",
    "print()\n",
    "print('classification report:\\n',classification_report(y_test,y_pred))\n",
    "print()\n",
    "print(\"confuion matrix:\\n\",confusion_matrix(y_test,y_pred))\n",
    "print()\n",
    "print(\"Accuracy score:\",accuracy_score(y_test,y_pred))\n",
    "print('precision score:',precision_score(y_test,y_pred))\n",
    "print(\"\\n\\nHeatmap on prediction:\\n\")\n",
    "sns.heatmap(confusion_matrix(y_test,y_pred),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UZm55vs6Thaj"
   },
   "outputs": [],
   "source": [
    "def check(text):\n",
    "    lab = ['not spam','spam'] \n",
    "    x = cv.transform([text]).toarray()\n",
    "    p = clf.predict(x)\n",
    "    s = [str(i) for i in p]\n",
    "    a = int(\"\".join(s))\n",
    "    res = str(\"This message is looking: \"+ lab[a])\n",
    "    print(res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ml2RaVr7To2a",
    "outputId": "13a310ef-967f-4945-8783-5657bceb1afc"
   },
   "outputs": [],
   "source": [
    "check('Congratulations, your entry into our contest last month made you a WINNER! goto our website to claim your price! You have 24 hours to claim.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GMV2tyeui4k8",
    "outputId": "450f105b-05b7-4648-963a-aadcda11f475"
   },
   "outputs": [],
   "source": [
    "check(\"I cannot attend the school for 10 days as I am going out on a vaccation kindly excuse me. I will keep my notes up to date as soon I return. Thanking you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7LwuyzAAT86r",
    "outputId": "44c1a8bf-99ce-44f5-bcdf-9251a4fe12bc"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(cv,'vectorizer.sav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mmgo3_UquFNe",
    "outputId": "aef6c232-536f-4eae-f3d1-33ad54e01917"
   },
   "outputs": [],
   "source": [
    "joblib.dump(clf,'model.sav')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "spam_minor(1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
